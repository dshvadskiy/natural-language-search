{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-2-4-bert-nearest-neighbor-search.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xjKlsE2GAUy",
        "colab_type": "text"
      },
      "source": [
        "#OSC NLP 2.4 - BERT Nearest Neighbor Search\n",
        "\n",
        "##Introduction\n",
        "\n",
        "In this lab, you'll build a \"search engine\" using BERT encodings with nearest neighbor algorithms.  You'll start with a basic working system and tune it on your own to improve search evaluation metrics against a test judgement set!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-3XegsGt8s",
        "colab_type": "text"
      },
      "source": [
        "##Install Dependencies\n",
        "\n",
        "We're using the transformers library, provided by https://huggingface.co\n",
        "\n",
        "Installation should only take a moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtVgVRxcyGet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCLEuF08G25L",
        "colab_type": "text"
      },
      "source": [
        "###Setup g-drive file access\n",
        "\n",
        "Be sure to set the ```path``` value below, to the same folder as where you uploaded the labs folder from this github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG2bsb0qijDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/labs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zM0KpGC2Nho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Grant access to your local g-drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW2wOInlIV1a",
        "colab_type": "text"
      },
      "source": [
        "### Import and initialize dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "996P-JoBYLgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygL-p42xasY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #Because we're using pytorch model architectures\n",
        "import json #Because we're loading blog-posts.json\n",
        "import numpy #Because we're working with tensors\n",
        "import tqdm #Because we'll see a pretty progress bar in the notebook\n",
        "from transformers import (\n",
        "  BertConfig,\n",
        "  BertTokenizer,\n",
        "  BertModel\n",
        ")\n",
        "device = torch.device(\"cuda:0\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "# Show the GPU that you're borrowing from Google.\n",
        "# Usually either a 'Tesla T4' or 'Tesla P100-PCIE-16GB'\n",
        "# If you get a different one, please let us know!\n",
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPwWRkq_Ieim",
        "colab_type": "text"
      },
      "source": [
        "###Load Content Data\n",
        "\n",
        "For this lab, we'll use the blog content from http://o19s.com/blog (data captured as of 2020-01-20)\n",
        "\n",
        "It contains titles, summaries (two to five sentences), and content (long written text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdG2vbSU8jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "posts = json.load(open(path + 'blog-posts.json'))\n",
        "print('Loaded',len(posts),'blog posts')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZaazQ61TCZZ",
        "colab_type": "text"
      },
      "source": [
        "##Tokenize\n",
        "\n",
        "We are now at the point where we need to convert the blog data into BERT encodings.  We'll use these encodings to build an index of vectors and metadata, that can be explored with nearest-neighbor, and other tensor based retrieval algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx4bNnRXTzt5",
        "colab_type": "text"
      },
      "source": [
        "###Using Huggingface Transformers\n",
        "\n",
        "We'll be using the library available here: https://huggingface.co/transformers/ \n",
        "\n",
        "For BERT, we're using the smaller ```bert-base-uncased``` model also provided by Huggingface.  You can learn more about this model here: https://huggingface.co/bert-base-uncased and in a list of all models here: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-nPcnWrxYtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "print('tokenizer and model are ready')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QsGEELIF-T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(text):\n",
        "  #For the text parameter, get the output encodings and metadata from the BERT tokenizer and encoder:\n",
        "  input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
        "  outputs = model(input_ids)\n",
        "  #The model is returned as a flat array, so we need to reshape it to N by 768, where N is the number of tokens that have been encoded, and 768 being the output encoding dimensions for one token.\n",
        "  flat = outputs[0][0].detach().numpy()\n",
        "  encodings = numpy.reshape(flat,(flat.size//768,768))\n",
        "  return encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtT0iqwd9qUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test the encoder!  You should get output similar to:\n",
        "\"\"\"\n",
        "[[-0.2130882  -0.12260079 -0.29699266 ... -0.3321391   0.7823828\n",
        "   0.17157161]\n",
        " [ 0.3834221   0.14510818 -0.48845595 ... -0.2496235   1.3287714\n",
        "   0.08275799]\n",
        " [ 0.32850048  0.7211096   0.57169473 ... -0.631611    0.7564233\n",
        "   0.48113757]\n",
        " ...\n",
        " [ 0.01067545 -1.0143113   0.69491935 ...  0.40127933  0.39096206\n",
        "  -0.52970445]\n",
        " [-0.42570952 -0.8559151   0.23692301 ...  0.36261156  0.24261396\n",
        "  -0.5436267 ]\n",
        " [ 0.20549215  0.31845802  0.30723798 ...  0.39721662  0.03817794\n",
        "  -0.0633641 ]]\n",
        "\"\"\"\n",
        "print(encode(posts[0]['summary']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu5v2iJ4VqDq",
        "colab_type": "text"
      },
      "source": [
        "###Build the index!\n",
        "\n",
        "Index all the things!  Take a guess as to how long this will take, for less than 700 blog post titles and summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al5lIrmr1qfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encodeIndex(posts):\n",
        "  index = []\n",
        "  for i in tqdm.tqdm(range(len(posts))):\n",
        "    post = posts[i]\n",
        "\n",
        "    title_encodings = encode(post['title'])\n",
        "    summary_encodings = encode(post['summary'])\n",
        "\n",
        "    \"\"\"\n",
        "     Uncomment at your own risk!\n",
        "     This will take a long while and fail due to an error during encoding time.\n",
        "     Bonus points: why will this happen?\n",
        "    \"\"\"\n",
        "    #content_encodings = encode(post['content']) \n",
        "\n",
        "    index.append({\n",
        "      \"id\":post['id'],\n",
        "      \"url\":post['url'],\n",
        "      \"title\":post['title'],\n",
        "      \"summary\":post['summary'],\n",
        "      \"title_encodings\":title_encodings,\n",
        "      \"summary_encodings\":summary_encodings\n",
        "      #\"content\":post['content'],\n",
        "      #\"content_encodings\":content_encodings\n",
        "    })\n",
        "  return index\n",
        "index = encodeIndex(posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIasFUVASfrg",
        "colab_type": "text"
      },
      "source": [
        "## Let's build an encoding similarity search engine!\n",
        "\n",
        "We will have a similarity function, and a search function.  Using the title_encodings and summary_encodings, make a new similarity function to replace 'weak_similarity', or change the boosts from the title and summary fields.  Who can make the best recall for k@10?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blCAzev7-D_O",
        "colab_type": "text"
      },
      "source": [
        "###Printing and Measuring methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6zCRnyLaAiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run print the top k results for all the queries!\n",
        "def print_results(querypair,results,metrics=[],k=10):\n",
        "  print('QUERY:',querypair[0])\n",
        "  print('SCORES:',metrics)\n",
        "  print('RELEVANT DOCS:',querypair[1])\n",
        "  for result in results[0:k]:\n",
        "    score = result[0]\n",
        "    doc = result[1]\n",
        "    if doc['url'] in querypair[1]:\n",
        "      found = 'FOUND!'\n",
        "    else:\n",
        "      found = '      '\n",
        "    print('..',found,score,doc[\"title\"],doc[\"url\"])\n",
        "  print('-----------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wJ4HvIBlYEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Relevance metrics!  Use these to score the results returned\n",
        "def precision(results,judgements,k=10):\n",
        "  tp=0\n",
        "  fp=0\n",
        "  if len(judgements)==0:\n",
        "    return 0.0\n",
        "  for result in results[0:k]:\n",
        "    if result[1]['url'] in judgements:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1\n",
        "  return tp/(tp+fp)\n",
        "\n",
        "#Relevance metrics!  Use these to score the results returned\n",
        "def recall(results,judgements,k=10):\n",
        "  tp=0\n",
        "  if len(judgements)==0:\n",
        "    return 0.0\n",
        "  for result in results[0:k]:\n",
        "    if result[1]['url'] in judgements:\n",
        "      tp+=1\n",
        "  return tp/len(judgements)\n",
        "\n",
        "def f1(p,r):\n",
        "  n = p*r;\n",
        "  d = p+r;\n",
        "  if d==0.0:\n",
        "    return 0.0\n",
        "  return 2*(n/d);\n",
        "\n",
        "def measure_results(results,judgements,k=10):\n",
        "  p = precision(results,judgements,k)\n",
        "  r = recall(results,judgements,k)\n",
        "  f = f1(p,r)\n",
        "  return p,r,f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiDgsQWU-HzT",
        "colab_type": "text"
      },
      "source": [
        "###The retrieval and scoring functions\n",
        "\n",
        "These functions are used to query the index, score the results, and return the ranked list.  Tune these functions to improve the relevance metrics!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G6gBWUh1igC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Our weak_similarity function takes the dot product of all tokens and averages them together\n",
        "def weak_similarity(encoding1,encoding2):\n",
        "  total = 0\n",
        "  dims = 0\n",
        "  for a in encoding1:\n",
        "    for b in encoding2:\n",
        "      total+=a.dot(b)\n",
        "      dims+=1\n",
        "  return total/dims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJuHYln2Jg1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Our search function takes a querystring, the index, and the similarity function\n",
        "#It returns a ranked resultset ranked by descending score\n",
        "def berty_searchy(querystring,index,similarity):\n",
        "  query_encodings = encode(querystring)\n",
        "  resultset = []\n",
        "  for i in range(len(index)):\n",
        "    record = index[i]\n",
        "    \n",
        "    #Get the similarity score between query and title\n",
        "    title_similarity = similarity(query_encodings,record[\"title_encodings\"])\n",
        "\n",
        "    #Get the similarity score between query and title\n",
        "    summary_similarity = similarity(query_encodings,record[\"summary_encodings\"])\n",
        "    \n",
        "    #This is the scorer that blends the similarities!\n",
        "    score = title_similarity * 1.2 + summary_similarity\n",
        "\n",
        "    resultset.append([score,record])\n",
        "\n",
        "  #Rerank the resultset by score descending\n",
        "  reranked = sorted(resultset, reverse=True, key=lambda k: k[0])\n",
        "  return reranked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXSd5bkXHo1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test the weak_similarity function with some comparissons:\n",
        "A = encode(\"Apples are very tasty.\")\n",
        "B = encode(\"Apple stock is high.\")\n",
        "C = encode(\"I bought a new iPhone today.\")\n",
        "D = encode(\"I ate some fruit.\")\n",
        "\n",
        "print(\"~(A,B) ==\", weak_similarity(A,B))\n",
        "print(\"~(B,C) ==\", weak_similarity(B,C))\n",
        "print(\"~(A,C) ==\", weak_similarity(A,C))\n",
        "print(\"~(A,D) ==\", weak_similarity(A,D))\n",
        "print(\"~(B,D) ==\", weak_similarity(B,D))\n",
        "print(\"~(C,D) ==\", weak_similarity(C,D))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTtV2IN8h7Po",
        "colab_type": "text"
      },
      "source": [
        "###Test Set\n",
        "\n",
        "This is our test set of ten queries and judgements of the most relevant documents per query.  We will use this set to evaluate relevance metrics of our similarity and scoring methods!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXNgt4-2rIup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "queries = [\n",
        "  (\"Why should I come to Haystack?\",[\"https://opensourceconnections.com/blog/2019/01/07/haystack-democratize-relevance/\",\"https://opensourceconnections.com/blog/2020/01/14/why-you-should-submit-a-talk-to-haystack-the-search-relevance-conference/\",\"https://opensourceconnections.com/blog/2018/04/10/haystack-search-relevance/\",\"https://opensourceconnections.com/blog/2019/04/24/haystack/\",\"https://opensourceconnections.com/blog/2018/01/04/why-a-search-relevance-conference/\"]),\n",
        "  (\"programming with java\",[\"https://opensourceconnections.com/blog/2008/01/02/is-java-the-new-cobol-2/\",\"https://opensourceconnections.com/blog/2015/12/22/exploring-custom-typecodecs-in-the-cassandra-java-driver/\",\"https://opensourceconnections.com/blog/2013/02/12/using-solr-join-to-find-the-best-time-to-ask-questions-on-stackoverflow/\"]),\n",
        "  (\"OCR example\",[\"https://opensourceconnections.com/blog/2019/12/03/using-tika-and-tesseract-as-an-api-exposed-by-solr-via-extractingrequesthandler/\",\"https://opensourceconnections.com/blog/2019/11/26/tika-and-tesseract-outside-of-solr/\",\"https://opensourceconnections.com/blog/2019/10/24/it-s-okay-to-run-tika-inside-of-solr-if-and-only-if/\",\"https://opensourceconnections.com/blog/2019/12/10/tesseract-3-and-tika/\"]),\n",
        "  (\"searching PDFs\",[\"https://opensourceconnections.com/blog/2019/10/01/solr-meetup-mimecast/\",\"https://opensourceconnections.com/blog/2019/12/17/parsing-tika-tesseract-output-inside-of-solr-via-statelessscriptupdateprocessorfactory/\",\"https://opensourceconnections.com/blog/2019/11/22/it-s-time-for-tika-tuesdays/\",\"https://opensourceconnections.com/blog/2019/10/24/it-s-okay-to-run-tika-inside-of-solr-if-and-only-if/\"]),\n",
        "  (\"learning tika\",[\"https://opensourceconnections.com/blog/2019/11/22/it-s-time-for-tika-tuesdays/\",\"https://opensourceconnections.com/blog/2019/11/26/tika-and-tesseract-outside-of-solr/\",\"https://opensourceconnections.com/blog/2019/12/03/using-tika-and-tesseract-as-an-api-exposed-by-solr-via-extractingrequesthandler/,https://opensourceconnections.com/blog/2013/04/28/indexing-millions-of-documents-using-tika-and-atomic-update/,https://opensourceconnections.com/blog/2019/10/24/it-s-okay-to-run-tika-inside-of-solr-if-and-only-if/,https://opensourceconnections.com/blog/2019/12/10/tesseract-3-and-tika/\"]),\n",
        "  (\"diversity in ecommerce\",[\"https://opensourceconnections.com/blog/2019/06/26/catching-mices-in-berlin-for-ecommerce-search/\",\"https://opensourceconnections.com/blog/2019/09/05/diversity-vs-relevance/\"]),\n",
        "  (\"morelikethis\",[\"https://opensourceconnections.com/blog/2019/05/02/london-solr-meetup-k8s-solr/\",\"https://opensourceconnections.com/blog/2016/09/13/search-engines-are-the-future-of-recsys/\",\"https://opensourceconnections.com/blog/2016/08/21/recommendations-systems-not-as-cool-as-friends/\",\"https://opensourceconnections.com/blog/2016/06/06/recommender-systems-101-basket-analysis/\",\"https://opensourceconnections.com/blog/2016/10/05/elastic-graph-recommendor/\",\"https://opensourceconnections.com/blog/2013/10/05/search-aware-product-recommendation-in-solr/\",\"https://opensourceconnections.com/blog/2013/07/04/friend-recommendations-using-mapreduce/\"]),\n",
        "  (\"tokenization and analyzers\",[\"https://opensourceconnections.com/blog/2015/09/18/the-simple-power-of-elasticsearch-analyzers/\",\"https://opensourceconnections.com/blog/2015/09/22/elyzer-step-by-step-elasticsearch-analyzer-debugging/\"]),\n",
        "  (\"Is Solr better than Elasticsearch?\",[\"https://opensourceconnections.com/blog/2015/12/15/solr-vs-elasticsearch-relevance-part-one/\",\"https://opensourceconnections.com/blog/2016/01/22/solr-vs-elasticsearch-relevance-part-two/\",\"https://opensourceconnections.com/blog/2019/02/28/stop-worrying-solr-elasticsearch/\",\"https://opensourceconnections.com/blog/2016/06/01/thoughts-on-algolia/\"]),\n",
        "  (\"quepid judgements\",[\"https://opensourceconnections.com/blog/2014/06/10/what-is-search-relevancy/\",\"https://opensourceconnections.com/blog/2019/07/25/2019-07-22-quepid-is-now-open-source/\",\"https://opensourceconnections.com/blog/2013/10/07/quepid-give-your-search-queries-some-love/\",\"https://opensourceconnections.com/blog/2015/07/15/quepid-v0.2.0-released/\"])\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeHv8igBiKka",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate!\n",
        "\n",
        "Run a \"search\" for each query, and evaluate the relevance metrics for the returned results.  Change the similarity method and the scorer to get a higher F1 metric!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L--M7GEMqt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(querypairs,k=10):\n",
        "  measurements = []\n",
        "  for querypair in querypairs:\n",
        "    query = querypair[0]\n",
        "    judgements = querypair[1]\n",
        "    results = berty_searchy(query,index,weak_similarity)\n",
        "    p,r,f = measure_results(results,judgements)\n",
        "    measurements.append([p,r,f])\n",
        "    print_results(querypair,results,metrics=[p,r,f],k=k)\n",
        "  print('---------------------------------------')\n",
        "  print('================================================')\n",
        "  print('TOTAL PRECISION ..',sum([p[0] for p in measurements])/len(measurements))\n",
        "  print('TOTAL RECALL    ..',sum([r[1] for r in measurements])/len(measurements))\n",
        "  print('TOTAL F1        ..',sum([f[2] for f in measurements])/len(measurements))\n",
        "\n",
        "# Run the search and evaluate the results!\n",
        "# 'k' is the number of top results that will be shown and evaluated\n",
        "evaluate(queries,k=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "986PeWIJNhXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}